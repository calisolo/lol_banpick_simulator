import requests
import time
import json
import os
from typing import List, Dict
from dotenv import load_dotenv
# import pathlib
# dotenv_path = pathlib.Path.home() / "code/.env"

dotenv_path = ".env"
load_dotenv(dotenv_path)
api_key = os.getenv("RIOT_KEY")

API_KEY = api_key  # ğŸ”‘ ì—¬ê¸°ì— ì‹¤ì œ API í‚¤ ì…ë ¥
BASE_URL = "https://kr.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5"

TIERS = ["PLATINUM"]#["DIAMOND", "EMERALD", "PLATINUM", "GOLD", "SILVER", "BRONZE", "IRON"]
DIVISIONS = ["III", "IV"]#["I", "II", "III", "IV"]

# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì„ íƒ)
os.makedirs("rank_data", exist_ok=True)

def save_entries(tier: str, division: str, entries: List[Dict]):
    filepath = os.path.join("rank_data", f"{tier}-{division}.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)
    print(f"âœ… Saved {len(entries)} entries to {filepath}")

def fetch_page(tier: str, division: str, page: int) -> List[Dict]:
    url = f"{BASE_URL}/{tier}/{division}"
    params = {
        "page": page,
        "api_key": API_KEY
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        if not isinstance(data, list):
            print(f"âš ï¸ Unexpected response (not list) at {tier}/{division}/page={page}: {data}")
            return []
        return data
    except requests.exceptions.RequestException as e:
        print(f"âŒ Request failed at {tier}/{division}/page={page}: {e}")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSON decode failed at {tier}/{division}/page={page}")
        return []

def crawl_tier_division(tier: str, division: str):
    print(f"\nğŸ” Starting {tier} {division}...")
    all_entries = []
    page = 1

    while True:
        print(f"  â†’ Fetching page {page}...", end=" ")
        entries = fetch_page(tier, division, page)

        if not entries:  # ë¹ˆ ë°°ì—´ì´ë©´ ì¢…ë£Œ
            print("[] â†’ End of pages.")
            break

        print(f"{len(entries)} entries")
        all_entries.extend(entries)

        # ìš”ì²­ ì œí•œ ì¤€ìˆ˜: 1ì´ˆ ëŒ€ê¸° (20/sec, 100/2min ì´ˆê³¼ ë°©ì§€)
        time.sleep(1.0)

        # ë‹¤ìŒ í˜ì´ì§€
        page += 1

    # ì €ì¥
    if all_entries:
        save_entries(tier, division, all_entries)
    else:
        print(f"âš ï¸ No data for {tier} {division}")

def main():
    print("ğŸš€ pid Crawler")
    print("âš ï¸  Rate limit: 20/1s, 100/2min â†’ Using 1 request/second for safety.")
    print("=" * 60)
    
    for tier in TIERS:
        for division in DIVISIONS:
            crawl_tier_division(tier, division)

    print("\nğŸ‰ All done!")

if __name__ == "__main__":
    main()